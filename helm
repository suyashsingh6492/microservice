helm is going to follow a packaging format called Charts. Inside Helm, a chart is a collection of files that describes a related set of Kubernetes resources.
if you have 50 microservices, you can club all these 50 microservices related manifest files into a single component called Chart Inside Helm.
helm charts is, a chart can have a child charts or dependent charts as well,
Helm is a package manager for Kubernetes.
Package manager is a component that can help you in installing or uninstalling or upgrading your software packages. Just like how we have helm for Kubernetes,
like npm, pip,
we are going to create a single template.yaml file for the Service object regardless of how many microservices you have inside your cluster,
at some places it is trying to accept the dynamic values. whatever you mention inside this double curly braces is going to represent a dynamic value
Helm is going to support, release or version management as well, which means whenever needed, you can roll back your entire Kubernetes cluster to the previous working state with a single command.
https://helm.sh/docs/intro/install/
help ls , if empty  this is going to list all the religions or all the installations that we have done into the Kubernetes cluster with the help of Helm.
Helm is going to look for Kubernetes cluster connection details inside your local system. it is going to make an entry inside your system.
go to suyashkumarsingh (user) > hidden folder .kube > config> open file , you'll be able to see all the connection details that my kubectl right now is using to connect to my Kubernetes cluster.
click on using helm > helm search >
https://helm.sh/docs/intro/using_helm/
helm search hub wordpress  , we need to search if there are any WordPress related charts available inside the public repositories
above command will search for all WordPress charts on the artifact hub.
helm install happy-panda bitnami/wordpress , install wordpress , under the Bitnami repo.
Bitnami is a famous repository which maintains production ready helm charts     ,we need to make sure we have added the Bitnami repo details inside our local system.
first add into local repo like == helm repo add brigade https://brigadecore.github.io/charts
so to add bitnami : helm repo add bitnami https://charts.bitnami.com/bitnami.
 try to create your own helm chart or you try to use the third party helm chart, it is going to follow the predefined structure.
 chart.yaml.: chart.yaml file is going to have meta information about the helm chart.
 values.yaml: we need to maintain all the dynamic values that we want to inject at runtime into the template files
  sub folder is charts: will have other helm charts which my current chart is dependent on
  templates: maintain all the required Kubernetes manifest template Yaml files so into this template
helm ls > list of installations that we have done with the help of Helm.
help uninstall <name>
to create helm chart go to helms folder and type: helm create bank-common  ,to build a helm chart that is going to be act as a common chart for all my microservices.
delete all files inside templates folder, remove content of values.yaml ,
create 3 file inside templates, service.yaml: this is a helm template but not a Kubernetes manifest file
So first I'm trying to define a name for this entire template. using  define ,
whatever hyphens that you see at the starting and at the end, those are helpful to trim any space that you may have before and after of your statement.
we should also close that by using this end statement.
under the metadata name, I need to inject a dynamic value.  whoever is going to use this template Yaml file, they need to provide a service name inside their values.yaml.
So values is a helm framework object. So inside that values object keys and values that you have defined inside your values.yaml will be available and to access them.
And similarly, under the specification selector app we are using a key which is app label inside the Values object
in deployment.yaml file
Now, after defining all the container related properties, we should also try to inject environment variables that are required for my particular microservice.
inside helm we can write the if tag by using this if .values. app name enabled.
So if this boolean value has true, then this entire environment variable is going to be injected into the particular microservice deployment yaml file.
maybe there might be some microservice where I don't have to pass the property, which is spring application name.
So the key inside the config map is going to be the spring profiles active only.
So based upon this key inside the config map, my helm is going to look up for the value. The same is going to be assigned to this environment variable which is spring profiles active.
in configmap.yaml:
So I'm going to follow a standard like wherever I mentioned a prefix value as global. So that property name is going to be common for all microservices inside my microservice network.
So whoever is going to leverage this common helm chart, they are going to provide their own values.yaml
bank-services % helm create accounts  , delete all templates content  , delete valus.yaml content , go to chart.yaml
change version to 1.0.0, add dependency key please mention the version of helm chart of bank-common.
past the file in templates folder , I'm just trying to refer other template that I have defined with the name common.deployment.
 we have defined this template inside the bank-common helm chart? I'm simply trying to refer to the template available inside the other helm chart,
 Please make sure this dot is also present because this is a syntax that we need to follow. now check values.yml file
 these values.yaml like you can see first I'm trying to mention a key with the name deployment name.
 the service type here I'm mentioning the cluster IP because I don't want to expose my accounts microservice to the outside world.
  resource server enabled as false.
 The reason is inside my microservices only Gateway server is going to act as a resource server accounts  microservice is not an resource server.
 chart folder is empty now it is empty because we have not compiled our helm chart as of now.
 go inside accounts and type: helm dependencies build , compile successful ,
 copy account and paste as card and loans and others too
create new folder environments > inside it > helm create dev-env , delete template, and values.yaml content
add configmap.yaml in templates folder,
we already defined a config map template inside the bank-common helm chart with the name as common.configmap.
the prefix value that I have mentioned for all these values is global  But this is not a mandatory or a standard from helm.
This is a prefix that I want to maintain for my own understanding.
So here the host name has to be same as your service name that you have defined. So the service name that we have defined inside the service.yaml file of configserver is config
 we are going to set up all the third party components like Keycloak, Kafka and Grafana components with the helm charts only. In that process they are going to be deployed with the production standards.
 So when we follow those production standards, this is how our service name is going to be built inside
 ok for now go inside environments>dev-env and type : helm dependencies build

you may want to know how your Kubernetes manifest files is going to look like that are going to be generated by your helm chart.
helm template .  ,the same Kubernetes manifest files are also going to be installed inside your Kubernetes cluster.
this dot  I'm telling my helm template is available in this folder only.

 since we have made some changes inside the bank-common chart. We need to recompile all the charts where they have dependency on the bank-common.
to build all dependency from bank service folder
cd helms
cd bank-common/
helm dependencies build
cd ../bank-services
cd accounts
helm dependencies build
cd ../cards
helm dependencies build
cd ../configserver
helm dependencies build
cd ../eurekaserver
helm dependencies build
cd ../message
helm dependencies build
cd ../loans
helm dependencies build
cd ../gatewayserver
helm dependencies build
cd ../../environments/dev-env
helm dependencies build
cd ../../..


 if you want to install Kafka inside your Kubernetes cluster, you don't have to prepare the Kubernetes manifest files manually.
 Instead, you can rely on the helm charts available inside the web. bitnami
 https://github.com/bitnami/charts/tree/main/bitnami
 copy the zip file and extract it , past the key cloak folder
 If I try to run this Keycloak helm chart into my local Kubernetes cluster by default it is going to deploy my keycloak service with a cluster IP.
 But since I want to access my keyclock to create the client details and roles information, I'm going to expose my key clock service as a load balancer.
open keycloack>  value.yaml and search for ClusterIP and chagne it to   type: LoadBalancer
search for adminPassword
So whenever my admin password is empty behind the scenes, my keycloak helm chart is going to create a secret which will have some random password.
so give password ,
I'm going to run a helm command, which is helm install and what is the name that you want to give for your keycloak installation.
So once you have given some name to your release, you can mention what is the chart name or what is the folder name where your chart is present.
from helms folder > cd keycloak  , helm dependencies build
cd ..
helm install keycloak keycloak
follow the insturction to get the url

  export HTTP_SERVICE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[?(@.name=='http')].port}" services keycloak)
    export SERVICE_IP=$(kubectl get svc --namespace default keycloak -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

    echo "http://${SERVICE_IP}:${HTTP_SERVICE_PORT}/"
now you will able to access url http://:80/ , or http://locahost:80
my keycloak also has dependency on other helm charts like Postgres because behind the scenes my keycloak is going to use Postgres database.

 this might you recieve in logs too
Keycloak can be accessed through the following DNS name from within your cluster:
    keycloak.default.svc.cluster.local (port 80)
So when we try to give these URL details to Gateway Server, it should be able to easily connect because my gateway server also is going to be deployed in the same Kubernetes cluster.

their might be memory problem > so go to docker > settings> resources > change cpu limit to 8 , memory to 12 gb,
run kubernative dashboard ui and check pods you will see keycloak is running
using  persistent volume claims, the pods can claim some space inside the worker nodes.
kubectl get pvc  , check all volums to delete pvc , kubectl delete pvc <name> ,
to install kafka, copy and paste kafka folder , go to value.yaml, set replicaCount =1
disable kafka security by SASL_PLAINTEXT. So this is the security protocol By default my Kafka is going to leverage.
change to PLAINTEXT,
go to kafka folder helm dependencies build , return back and run helm install kafka kafka, check logs and steps
So here in the output, you can see there is an information like each Kafka broker can be accessed by producers via Port 9092 by using the DNS name.
for rabbit change the poassword to guest , username to guest, in value.yaml
and change   type: ClusterIP to   type: LoadBalancer
for rabbitmq go to rabbitmq folder > helm dependencies build , cd .. , run> helm install rabbit rabbitmq
you will see message RabbitMQ can be accessed within the cluster on port 5672 at rabbit-rabbitmq.default.svc.cluster.local
http://localhost:15672/

to have prometheus > copy kube-prometheus ., open value.yaml , search for the additionalScrapeConfigs,
using these configs only my Prometheus will decide to scrape the metrics from the microservices.
change to  enabled: true, change     type: external to     type: internal
we need to provide the details of our microservices using which my Prometheus can connect with them and read the metrics of them
search for joblist and update it, see the value
go to kube-prometheus > helm dependencies build  , cd .. , helm install prometheus kube-prometheus
So by default Prometheus is set up with the help of cluster IP, so we cannot really access it.
But for some reason, if you want to access the same, So using the kubectl port forward command, we can temporarily expose a service.
mentioned in o/p logs too
     echo "Prometheus URL: http://127.0.0.1:9090/"
       kubectl port-forward --namespace default svc/prometheus-kube-prometheus-prometheus 9090:9090

 if you go to see the target http://127.0.0.1:9090/targets?search=  ,
 As of now, my accounts cards, config server, Eureka server all of them we didn't setup. That's why the status for all of them is showing as in red color.

So before we try to set up the actual Grafana, first we need to make sure we set up the loki and tempo. For the same,
copy grafana-tempo, grafana-loki into the folder ,
First, let me try to set up grafana-loki because loki is responsible to aggregate all the logs by my individual microservices.
So here you can see my loki has dependency on other charts like Memcached. That's why it is trying to download all those dependencies.
cd grafana-loki > helm dependencies build , cd .. , run> helm install loki grafana-loki
So this will install many components inside my Kubernetes cluster like ingested, distributor, querier, promtail, compactor, gateway.
Can you imagine setting up all these without helm chart?
Your Kubernetes administrator will need a lot of help from the developers and the Grafana admin to set
up these and it may need months of effort to have a proper setup, whereas with Helm it is super, super easy.
now
 cd grafana-tempo/ open value.yaml > search for otlp , So these are related to open telemetry configurations.
 So as of now you can see the open telemetry communication with the help of Http protocol and gRPC protocol is disabled.
 enable Then only the open telemetry Java agent that we have inside our individual microservices. It can send the tracing details to the tempo.

  cd grafana-tempo/ >  helm dependencies build , cd .. , run> helm install tempo grafana-tempo

 But here we have a problem, which is our individual microservices need a tempo url to which my open telemetry is going to send the details of tracing details.
type :  kubectl get services

So the service that our open telemetry should connect is these distributor. tempo-grafana-tempo-distributor
port no is 4317 ,
So this is how we are establishing a link between our individual microservice open telemetry with the grafana tempo.

now install grafana
open value.yaml file So let me open this inside this values.yaml I need to look for the information on how to provide the
data source Details of tempo Loki and Prometheus.
We can provide these data source details to grafana using Yaml configurations, or we can also manually set up the data sources from the UI of the Grafana.
search for datasources : secretDefinition , see the value self
So this is the DNS name of Prometheus inside my Kubernetes cluster.
So once we define these values now, my grafana should be able to connect with all the components related to Prometheus, loki and tempo.

cd grafana  > ls , cd .. , run> helm install grafana grafana
my Grafana is going to be exposed as a cluster IP service.
So whenever I have some requirements to debug any issues, my Kubernetes admin can run a command which is related to kubectl port forward.
  echo "Browse to http://127.0.0.1:8080"
    kubectl port-forward svc/grafana 8080:3000 &
Because we expose that traffic at the port 8080.
So Grafana internally is going to start at the port 3000. But here the traffic is going to be exposed at the port 8080.
 kubectl port-forward svc/grafana 3000:3000 &
 access using http://127.0.0.1:3000/login ,
 to get user name and password , open new terminal ,

    echo "User: admin"
    echo "Password: $(kubectl get secret grafana-admin --namespace default -o jsonpath="{.data.GF_SECURITY_ADMIN_PASSWORD}" | base64 -d)"
admin , cnHre1W40Z
go to http://127.0.0.1:3000/explore , you will see prometheus , loki , tempo

now helm ls, This will show you all the releases or all the installations or deployments that we have done with the help of Helm.

now go to cd environments
First, we need to decide under which environment we want to deploy our microservices. what is the chat name.: dev-env
so do : helm install <release name> <chart name>
helm install bank dev-env
you get the output deployed , let's go to the Kubernetes dashboard. on new terminal
kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443

so final is if want to reprocess then first >  helm uninstall bank

cd helms
cd bank-common/
helm dependencies build
cd ../bank-services
cd accounts
helm dependencies build
cd ../cards
helm dependencies build
cd ../configserver
helm dependencies build
cd ../eurekaserver
helm dependencies build
cd ../message
helm dependencies build
cd ../loans
helm dependencies build
cd ../gatewayserver
helm dependencies build
cd ../../environments/dev-env
helm dependencies build
cd ../../..
cd helms
cd keycloak
helm dependencies build
cd ..
cd kube-prometheus
helm dependencies build
cd ..
cd rabbitmq
helm dependencies build
cd ..
cd grafana-tempo
helm dependencies build
cd ..
cd grafana-loki
helm dependencies build
cd ..
cd grafana
helm dependencies build
cd ..
helm install rabbit rabbitmq
helm install keycloak keycloak
helm install prometheus grafana-prometheus
helm install loki grafana-loki
helm install tempo grafana-tempo
helm install grafana grafana
cd environments
helm install bank dev-env
cd ../..


we saw how to roll out and roll back changes with the help of Kubernetes and Kubectl Command.
helm upgrade <releasename> ,
before this after making changes do helm dependencies build
ex: cd helms/environments  , helm upgrade bank dev-env
So if I try to execute this, my helm is smart enough to identify what are the changes and deploy the same changes into the Kubernetes cluster. And it is also going to give the revision number as two.
The beauty of helm rollback is it is going to rollback your entire Kubernetes cluster components to the previous desired state.
Whereas with Kubectl, if you want to roll back, you need to roll back component by component.
helm history <release name>
helm history bank
So now think like we want to roll back to the previous working version, which is revision one
helm rollback bank 1
again type : helm history bank
you will see     dev-env-0.1.0   1.0.0           Rollback to 1

let me show you the what are all the installations that we have done with the help of helm. helm ls
then do
helm uninstall bank
helm uninstall grafana
helm uninstall tempo
helm uninstall loki
helm uninstall prometheus
helm uninstall rabbit
helm uninstall keycloak
helm ls
also delete persistentvolumeclaim manually

One of the famous product, which is a competitor for Helm is customize.
we have here is Helm template, and with the help of Helm Template Command,
we can try to render the all the Kubernetes manifest files that your helm is going to use behind the scenes



where my accounts microservice is trying to ask Eureka Server like please give me the details of the loan service as a response to
the step two inside the step three, my Eureka server is going to give you all the IP address or the instance details of the loans microservice
my Eureka server has given two IP address details where my loans microservice is available.
my accounts microservice is going to have a problem,  The problem is it has to decide which instance, it has to forward the request.
it is going to do the load balancing with the help of spring cloud load balancer and after
the process of load balancing, it is going to choose one of the instance details and to the same the request is going to be sent in the step four.
 accounts microservice is itself is responsible for the load balancing. That's why we call this approach as client side load balancing and client side service discovery.
 The disadvantage is the developers they have to manually maintain the Eureka server
 They have to convert that as an Eureka server post that they should also make changes in all the microservices to connect with the Eureka Server
 can be avoided if we follow the server side service discovery and server side load balancing.
  you are not going to have any control on the load balancing.  you don't have to make any changes inside your microservices.

  We can follow the server side service discovery and load balancing only if we are using Kubernetes cluster
  Inside this approach, there will be a service discovery inside your Kubernetes cluster, which is responsible
  to monitor all the application instances and maintaining the details of them.
  the Discovery server itself uses the Kubernetes APIs to fetch details about the Kubernetes services and endpoints of all the microservices.
  So my Kubernetes Discovery server is going to query Kubernetes API to fetch all the details of the loans microservice instances.
  the only disadvantage with this approach is, the clients application are the developers will not have any control on the load balancing.
  We as a developers are the client microservice. We can simply forward the request to the service URL and the load balancing strategy or algorithm is completely decided by the Kubernetes cluster,

  search for spring cloud kubernates
  https://docs.spring.io/spring-cloud-kubernetes/reference/spring-cloud-kubernetes-discoveryserver.html

So whenever you define kind as list inside your Kubernetes manifest file, you can create any number of Kubernetes objects
the port that it is going to expose to the other microservices is 80, whereas it is going to start internally at the port 8761..
And the service type is ClusterIP.
So with the help of this role binding, we are going to bind a role to the service account.
But here, under the resources, they only give access to the service and endpoints.
So basically this Spring Cloud Kubernetes team, they built a spring boot application which is going to act as a discovery server inside your Kubernetes cluster.
So for these initial delay seconds, I have passed a value which is 100,  my Kubernetes is going to wait for the 100 seconds before it tries to perform the readiness probe very first time.
With the help of period seconds, we are telling to the Kubernetes cluster, please validate the readiness probe every 30s like initially it will try to perform the initial readiness probe after 100 seconds
There are two reasons on why I'm not using helm chat here. The very first one is Discovery Server is a one time setup inside your Kubernetes cluster.
With that reason, manually running these manifest files should be fine because this is only one time activity.
the second reason is as of now there is no helm chat developed by the helm chat community, including Bitnami for these discovery server.
cd kubernete > kubectl apply -f kubernetes-discoveryserver.yml

As a next step, we need to make some code changes inside our microservices to remove the Eureka related
changes on top of that, we should also make some changes inside our microservices to leverage the discovery server exposed by the Kubernetes.
use spring-cloud-starter-kubernetes-discoveryclient instead of spring-cloud-starter-netflix-eureka-client ,
As soon as I try to do a build, I'm getting an error inside my CustomerController.
Seems whenever I deleted the Eureka client related dependency, it is internally is also going to delete the interrelated dependencies.
add EnableDiscoveryClient in main class,in yml file comment out eureka configuration ,
property which is spring.cloud.kubernetes.discovery.all-namespaces as true.
, to make sure that our application is able to discover the services present inside other namespaces as well

Now as a next step, let me go to the location where we are using feign client to connect with the other microservices inside my accounts microservice.
CardFeignClient ,
Using the same application name my accounts microservice can fetch the details of all the instances
But in the scenario of Discovery server, we are not providing any integration details between our accounts, microservice and Discovery Server.
that's why my feign clients cannot really understand what is the service name or the host name or the dns name or the port number where it has to forward the request.
url at which my cards microservice is available
So what is the name that we have given for the cards microservice service. It is the cards
where all my cards microservice containers are going to be available at the Port 9000.
And here the Feign client is not going to perform any load balancing. It is simply going to forward the request to this URL inside the Kubernetes cluster.
it is not going to work because outside of your Kubernetes cluster there is no hostname or dns name with the name cards.
I'll go to the cards and here also first I'll open the pom.xml In the place of Eureka,
we are going to add a new dependency with the name Spring Cloud starter Kubernetes Discovery client.
CardsApplication we need to mention an annotation which is @EnableDiscoveryClient
go to application .yml remove eureka property and add kubernative property
Inside the config server, we are nowhere connecting with the Eureka server. So there is no need of any changes
same for message,
in gateway server,@EnableDiscoveryClient. and build.gradle discoveryclient ,
if you try to scroll down as of now, you can see the routing configurations that we have done here is going to have a uri configuration
lb://ACCOUNTS and lb://LOANS
So with this we are telling to the Gateway server, please leverage the spring cloud load balancer and do the load balancing at the Gateway server itself.
But we don't want that to happen because we are no more using the Eureka.
 we need to update these uri details with the actual URL details of your individual microservice.
 .uri("http://accounts:8080"))
 disable     gateway:
               discovery:
                 locator:
                   enabled: false
                   lowerCaseServiceId: true
property value as spring.cloud.kubernetes. discovery.enabled as true and at the same time discovery.allnamespace as true.
property is spring.cloud.discovery.client.health indicator enabled as false. So by default there is an health indicator bean is going to be created internally.

so finally from bank service we need to do this after building all the application loans, cards, account, config server, gateway server, message
So without mentioning that discovery server URL, how my gateway server accounts, loans and cards will know.
go to bank-common folder > templates> configmap.yaml , add SPRING.CLOUD.KUBERNETES.DISCOVERY.DISCOVERY-SERVER-URL property
If you go and check the Kubernetes manifest file that we have used to deploy the Discovery server here
we have name as  spring-cloud-kubernetes-discoveryserver

open helm accoutns-service , values.yaml, replicaCount: 2 ,
open environment folder >dev-env> Chart.yaml .,comment eureka dependency
delete chart.lock file from all services
cd kubernete
kubectl apply -f kubernetes-discoveryserver.yml
cd  accounts-service
docker build . -t suyashs52/accounts-service:v1
cd ../cards-service
docker build . -t suyashs52/card-service:v1
cd ../loan-service
docker build . -t suyashs52/loan-service:v1
cd ../config-server
docker build . -t suyashs52/configserver:v1
cd ../eureka-server
docker build . -t suyashs52/eureka-server:v1
cd ../gateway-server
docker build . -t suyashs52/gateway-server:v1
cd ..
cd message
gradle jib
cd ..
 docker image push docker.io/suyashs52/accounts-service:v1
 docker image push docker.io/suyashs52/card-service:v1
 docker image push docker.io/suyashs52/loan-service:v1
 docker image push docker.io/suyashs52/configserver:v2
 docker image push docker.io/suyashs52/eureka-server:v1
 docker image push docker.io/suyashs52/gateway-server:v1
cd helms
cd keycloak
helm dependencies build
cd ..
cd kube-prometheus
helm dependencies build
cd ..
cd rabbitmq
helm dependencies build
cd ..
cd grafana-tempo
helm dependencies build
cd ..
cd grafana-loki
helm dependencies build
cd ..
cd grafana
helm dependencies build
cd ..
helm install rabbit rabbitmq
helm install keycloak keycloak
helm install prometheus grafana-prometheus
helm install loki grafana-loki
helm install tempo grafana-tempo
helm install grafana grafana
cd ..
cd helms
cd bank-common/
helm dependencies build
cd ../bank-services
cd accounts
helm dependencies build
cd ../cards
helm dependencies build
cd ../configserver
helm dependencies build
cd ../eurekaserver
helm dependencies build
cd ../message
helm dependencies build
cd ../loans
helm dependencies build
cd ../gatewayserver
helm dependencies build
cd ../../environments/dev-env
helm dependencies build
cd ../../..
cd helms/environments
helm install bank dev-env
cd ../..


---------------to check template validation do this
cd helms/environments/dev-env
helm template .
cd ../../..
--------------to check the kubernative pods on ui run on new terminal
kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443
https://localhost:8443/#/login





after finish
cd helms/environments
helm uninstall bank
cd ../..


Please note that I hope you remember that we created two replicas of accounts microservice and both
of them is going to have their own in-memory H2 database.
so use localhost:8072/bank/accounts/api/create and create 1 account , and use fetch, sometime use get error
my Kubernetes, which is responsible to the load balancing, is going to forward for one of the accounts
instance where the account is not available inside the internal H2 database and that will confirm the load balancing is also working fin

So if you are not getting these not found error, there is a good chance that your Kubernetes cluster
is maintaining the sticky session. So what is sticky session?
So whenever a request is coming from the same client using the same IP address from the same server
or from the same system, it is always going to forward the request to the same pod,
if I keep sending the send button, I'll keep getting the not found always because my Kubernetes
 is trying to remember the details of the client who is invoking this API.

 Post that you can try to invoke the API by using the browser in an incognito mode.

uninstall everything
helm uninstall bank
helm uninstall grafana
helm uninstall tempo
helm uninstall loki
helm uninstall prometheus
helm uninstall rabbit
helm uninstall keycloak
kubectl delete -f kubernetes-discoveryserver.yml
kubctl get pods
kubectl delete pod  spring-cloud-kubernetes-discoveryserver-deployment-69dcdd695sgk
kubectl delete service spring-cloud-kubernetes-discoveryserver