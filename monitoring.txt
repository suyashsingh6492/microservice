because many containers and many services, they are going to generate the logs inside their own containers.
Sometimes your request may travel more than 20 microservices to give a response, so in such scenarios
it is going to be very tedious job to look for the logs inside the containers individually.
how we can combine all the logs from the multiple services into a centralized location.  how are we going to monitor performance of our service calls?
 call through the microservice network and see how long it took to complete at each microservice.
  it's not going to be a feasible option to monitor all the microservices 24 by seven.
 That's why we should automatically trigger some alerts and notifications, if there is some abnormal behavior inside a particular microservice.

 Observability is the ability to understand the internal state of a system by observing or by understanding its output.
  can be achieved by collecting and analyzing data from a variety of sources such as metrics, logs and traces.
  three pillars for observability. metric: cpu usage, memory usage and response times.
   we have is logs. :Logs are a record of events that occur inside a system.
  traces.: Traces are a record of the path that a request takes through a system inside our microservices network.

  Monitoring in microservice context involves checking the telemetry data available for the application and defining the alerts for known failure state.
  we can build some dashboards and build alerts and notifications based upon metrics, logs and traces information.
  If the CPU utilization of a particular container or a microservice crossed more than 80%, then I want a alert to be triggered.
  we can also track the health of our microservices. Using dashboards, alerts and notifications we can easily understand which
  microservice is underperforming or which microservice is having some network problems or any other kind
we can try to optimize our microservices by onboarding more number of instances or by killing the microservice, which is having problems.

Logs: logs contain a timestamp that indicates when the event happened, as well as the information about the event and its context.
    We should only log the severe events like there is an exception happened or there is an error happened inside production.
 So what is Grafana?:  if you want to implement log aggregating system, then go for the tool Grafana Loki.
  for tracing information you can leverage Grafana tempo.
  Grafana Loki is a horizontally scalable, highly available log aggregation system. It is designed to store any amount of logs from your microservices and applications.
  Alloy: responsible for scraping log lines,
  https://grafana.com/docs/loki/latest/get-started/quick-start/
  alloy to read the new logs whenever generated by the log app and collect the same logs and sent to loki
  In between there is an edge server like a gateway (nginx) .
  Inside Loki there are two components like Loki read component and Loki write component.
  Once my Loki component receives the logs, it is going to store the logs in a component with the name Minio.
  if one of the developer wants to understand the logs of a particular microservice. go to the grafana and he will try to search the logs
  request will go to the gateway  The gateway will forward the same request to the Loki read component  this log read component is going to read the logs from the minio and eventually
the same will be sent back to the Grafana

https://grafana.com/docs/loki/latest/get-started/quick-start/ , d
https://raw.githubusercontent.com/grafana/loki/main/examples/getting-started/docker-compose.yaml
in docker compose We are trying to tag to the network, which is Loki and we are also trying to create an alias name for this network Loki.
whenever we see & that indicates anchor. &loki-dns
we are trying to create a variable name, loki-dns we are trying to assign the value present under the networks.
    <<: *loki-dns we are going to merge this variable under this networks. With that, the same details that we have mentioned here will come under the write service as well.
/etc/alloy/config.alloy:ro  , read only config.alloy , don't change it
minio : create a directory inside the local host where we have this docker-compose.yml.  mkdir -p /data/loki-data
 to establish the link between Grafana and Loki. We need to provide this data source details to the grafana like where the Loki is deployed.
 gateway depend on read and write (loki read/loki write)
 flog we have a app with the name flog which will continuously generate some random logs.

 alloy related yml config
 https://raw.githubusercontent.com/grafana/loki/main/examples/getting-started/alloy-local-config.yaml
 alloy will read the log using socket unix:///var/run/docker.sock going to read the content from my docker containers with the help of the socket paths present inside all my containers
  the refresh interval is going to be five seconds
  whatever data my ally is going to read, I have defined with the help of regex
   That means I want my alloy to read all kind of logs data and the target label is going to be container.

 https://raw.githubusercontent.com/grafana/loki/main/examples/getting-started/loki-config.yaml
 make folder observability in docker compose folder.

 comment out redis related config
after starting all container go to http://localhost:3000/?orgId=1 
go to connection>data sources > http://localhost:3000/connections/datasources
click and check all connection detail mentioned in docker file. 
go to explore> select cotainer 
http://localhost:3000/explore
in promtail-local-config we define to take all log of container by regex 
in value you will see all container . select account-ms and run query
If needed, you can click on this live streaming with that any logs that are being generated behind by your accounts micro service container, they will immediately come here after every five seconds. 
search for bank-correlation-id in container, gateway-server-ms
all logs data save by minio service under .data folder in current dir you can see in docker compose vol secs. 

So to properly monitor our microservices, we need to understand metrics details like what is the
CPU usage, what is the memory usage, what is the heap dump usage, what are the threads, connections errors.
how to achieve this with the help of components like Spring Boot Actuator, Micrometer, Prometheus and Grafana?
first component that is responsible to generate the metrics inside a microservice instance is spring boot actuator.
my spring boot actuator is going to expose all the metrics related to the application.  /actuator/metrics
 Prometheus, : component which is responsible to extract and aggregate all the metrics from the microservices
 cannot understand the metrics provided by the actuator because actuator exposes the metrics information in a Json format which we can easily understand as
 humans, since the same format cannot be understand by the Prometheus, we need to use micrometer.
 Micrometer automatically exposes the actuator/metricsdata into a format that a particular monitoring system can understand.
 We have a design pattern with the name fact design pattern. It is going to deploy a front facing interface that will handle a lot of complexity behind the scenes.

